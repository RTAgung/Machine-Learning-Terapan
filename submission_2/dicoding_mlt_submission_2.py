# -*- coding: utf-8 -*-
"""Dicoding_MLT_Submission_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IcOK7m7GMs8giA5VwP9K1iJ-WGqVc1WX

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os

import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf

# %matplotlib inline

"""# Load Data

### Download data
"""

os.remove('anime-recommendation-database-2020.zip')
! kaggle datasets download -d hernan4444/anime-recommendation-database-2020

"""### Read CSV"""

# data source: https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020

path_anime = 'data/anime.csv'
path_animelist_user = 'data/animelist.csv'
path_rating = 'data/rating_complete.csv'
path_status = 'data/watching_status.csv'

anime = pd.read_csv(path_anime)
animelist_user = pd.read_csv(path_animelist_user)
rating = pd.read_csv(path_rating)
status = pd.read_csv(path_status)

"""# Data Understanding

Macam-macam file yang ada:
- `anime.csv` contain general information of every anime (17.562 different anime) like genre, stats, studio, etc.
- `animelist.csv` have the list of all animes register by the user with the respective score, watching status and numbers of episodes watched. This dataset contains 109 Million row, 17.562 different animes and 325.772 different users.
- `rating_complete.csv` is a subset of animelist.csv. This dataset only considers animes that the user has watched completely (watching_status==2) and gave it a score (score!=0). This dataset contains 57 Million ratings applied to 16.872 animes by 310.059 users.
- `watching_status.csv` describe every possible status of the column: "watching_status" in animelist.csv.
"""

print('Jumlah data anime: ', len(anime))
print('Jumlah data list anime dengan user: ', len(animelist_user))
print('Jumlah data rating anime: ', len(rating))
print('Jumlah data status user: ', len(status))

"""### Analisis data pada anime.csv"""

anime.info()

print("Jumlah anime:", len(anime.MAL_ID.unique()))

"""cek data null"""

anime.isnull().sum()

"""cek duplicated data"""

anime.duplicated().sum()

"""cek data pada genre anime"""

all_genres = []
anime_genres = anime.Genres.unique()

for i, v in enumerate(anime_genres):
  all_genres.extend(v.split(', '))

all_genres = sorted(set(all_genres))

print("Jumlah genre:", len(all_genres))
print("List genre:", all_genres)

total_genre_by_anime = {g: 0 for g in all_genres}

for anime_genre in anime['Genres']:
  list_anime_genre = anime_genre.split(', ')
  for genre in list_anime_genre:
    total_genre_by_anime[genre] = total_genre_by_anime[genre] + 1

total_genre_by_anime = dict(sorted(total_genre_by_anime.items(), key=lambda x:x[1], reverse=True))

plt.figure(figsize=(18,10))
sns.barplot(x = list(total_genre_by_anime.keys()),
            y = list(total_genre_by_anime.values()))
plt.xticks(rotation=90)
plt.title("Diagram Top Genre Seluruh Anime")
plt.show()

# top 10 genre
pd.DataFrame({
    "Genre": list(total_genre_by_anime.keys()),
    "Total": list(total_genre_by_anime.values())
}).head(10)

"""### Analisis data pada animelist.csv"""

animelist_user.info()

"""cek data null"""

animelist_user.isnull().sum()

"""cek duplicated data"""

animelist_user.duplicated().sum()

print("Jumlah aktivitas user terhadap anime:", len(animelist_user.user_id.unique()))
print("Jumlah anime:", len(animelist_user.anime_id.unique()))

sorted(animelist_user.rating.unique())

# top rating
total_rating_user = animelist_user.rating.value_counts()
pd.DataFrame({
    "Rating": list(total_rating_user.keys()),
    "Total": list(total_rating_user)
})

plt.figure(figsize=(12,6))
sns.barplot(x = list(total_rating_user.keys()),
            y = list(total_rating_user))
plt.title("Diagram Top Rating Seluruh User (dalam satuan 10 juta)")
plt.show()

sorted(animelist_user.watching_status.unique())

"""watching status hanya bernilai 0, 1, 2, 3, 4, 6.
Maka nilai 5, 33, 55 merupakan error, jadi perlu dihapus
"""

animelist_user.drop(animelist_user[animelist_user.watching_status == 5].index, inplace=True)
animelist_user.drop(animelist_user[animelist_user.watching_status == 33].index, inplace=True)
animelist_user.drop(animelist_user[animelist_user.watching_status == 55].index, inplace=True)

# top watching status
total_watching_status_user = animelist_user.watching_status.value_counts()
pd.DataFrame({
    "Watching Status": list(total_watching_status_user.keys()),
    "Total": list(total_watching_status_user)
})

plt.figure(figsize=(12,6))
sns.barplot(x = list(total_watching_status_user.keys()),
            y = list(total_watching_status_user))
plt.title("Diagram Top Watching Status Seluruh User (dalam satuan 10 juta)")
plt.show()

"""### Analisis data pada rating_complete.csv

This dataset only considers animes that the user has watched completely (watching_status==2) and gave it a score (score!=0)
"""

rating.info()

"""cek data null"""

rating.isnull().sum()

"""cek ducplicated data"""

rating.duplicated().sum()

print("Jumlah aktivitas user terhadap anime:", len(rating.user_id.unique()))
print("Jumlah anime:", len(rating.anime_id.unique()))

# top rating
total_rating_complete_user = rating.rating.value_counts()
pd.DataFrame({
    "Rating": list(total_rating_complete_user.keys()),
    "Total": list(total_rating_complete_user)
})

plt.figure(figsize=(12,6))
sns.barplot(x = list(total_rating_complete_user.keys()),
            y = list(total_rating_complete_user))
plt.title("Diagram Top Rating Seluruh User (dalam satuan 10 juta)")
plt.show()

"""### Analisis data pada watching_status.csv"""

status.info()

status

"""# Data Preparation

Memilih data yang akan digunakan
"""

df_anime = anime[["MAL_ID", "Name", "Score", "Genres", "Premiered"]]
df_anime.columns = ['anime_id', 'name', 'score', 'genres', 'premiered']

df_rating = rating

df_anime.head()

df_rating.head()

print("Jumlah anime yang digunakan:", len(df_rating.anime_id.unique()))
print("Jumlah user yang digunakan:", len(df_rating.user_id.unique()))
print("Jumlah rating user yang digunakan:", len(df_rating))

"""karena data terlalu besar, training model dilakukan hanya mengambil data sebanyak 1000 user saja"""

user_ids = np.array(df_rating.user_id.unique().tolist())

np.random.seed(42)
np.random.shuffle(user_ids)
user_ids = user_ids[:1000]

df_rating = df_rating[df_rating.user_id.isin(user_ids)]

print("Jumlah anime yang digunakan:", len(df_rating.anime_id.unique()))
print("Jumlah user yang digunakan:", len(df_rating.user_id.unique()))
print("Jumlah rating user yang digunakan:", len(df_rating))

"""cek data hubungan anime dengan rating, apakah semua rating anime memiliki detail anime"""

df_merge = pd.merge(df_rating, df_anime[['anime_id', 'name']], on='anime_id', how='left')
df_merge.head()

df_merge.isna().sum()

"""cek data hubungan anime dengan rating, apakah semua detail anime memiliki rating"""

df_merge2 = pd.merge(df_anime[['anime_id', 'name']], df_rating, on='anime_id', how='left')
df_merge2.head()

df_merge2.isna().sum()

"""terdapat 8763 data detail anime yang tidak memiliki rating"""

unused_anime_id = df_merge2[df_merge2.rating.isna()].anime_id.unique().tolist()

df_anime = df_anime[~(df_anime.anime_id.isin(unused_anime_id))]
print("Jumlah detail anime yang digunakan:", len(df_anime.anime_id.unique()))

"""save final data"""

df_rating.to_csv('final_rating.csv', index=False)
df_anime.to_csv('final_anime.csv', index=False)

"""load final data"""

df_rating = pd.read_csv('final_rating.csv')
df_anime = pd.read_csv('final_anime.csv')

df_rating

"""encoding data"""

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df_rating['user_id'].unique().tolist()
 
# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
 
# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = df_rating['anime_id'].unique().tolist()
 
# Melakukan proses encoding anime_id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
 
# Melakukan proses encoding angka ke anime_id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# Mapping user_id ke dataframe user
df_rating['user'] = df_rating['user_id'].map(user_to_user_encoded)
 
# Mapping anime_id ke dataframe anime
df_rating['anime'] = df_rating['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)
 
# Mengubah rating menjadi nilai float
df_rating['rating'] = df_rating['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df_rating['rating'])
 
# Nilai maksimal rating
max_rating = max(df_rating['rating'])
 
print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

# Mengacak dataset
df_rating = df_rating.sample(frac=1, random_state=42)
df_rating

"""splitting data"""

# Membuat variabel x untuk mencocokkan data user_id dan anime_id menjadi satu value
x = df_rating[['user', 'anime']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df_rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * df_rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x[:10], y[:10])

print(f'Total # of sample in whole dataset: {len(x)}')
print(f'Total # of sample in train dataset: {len(x_train)}')
print(f'Total # of sample in test dataset: {len(x_val)}')

"""# Model Development"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = tf.keras.layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.user_bias = tf.keras.layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = tf.keras.layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.anime_bias = tf.keras.layers.Embedding(num_anime, 1) # layer embedding anime bias
    self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu) # layer dense
    self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid) # layer output activation sigmoid
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2) 
 
    x = dot_user_anime + user_bias + anime_bias
    x = self.dense1(x)
    x = self.dense2(x)
    return x

def create_model(optimizer, embedding_size):
    tf_model = RecommenderNet(num_users, num_anime, embedding_size)
    tf_model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = optimizer,
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )
    return tf_model

# Memulai training
import warnings
warnings.filterwarnings('ignore')

model_params = {
    'optimizer': ['Adam', 'RMSprop'],
    'embedding_size': [50, 100]
}

model = {}
history = {}
params = {}

print("Training Parameter: ", model_params)

k = 1
for i in range(2):
    for j in range(2):
        opt = model_params['optimizer'][i]
        emb_size = model_params['embedding_size'][j]

        print("Start Training with optimizer: {} embedding_size: {}".format(opt, emb_size))
        model["model_"+str(k)] = create_model(
            optimizer=opt,
            embedding_size=emb_size,
        )

        history["model_"+str(k)] = model["model_"+str(k)].fit(
            x = x_train,
            y = y_train,
            batch_size = 64,
            epochs = 30,
            validation_data = (x_val, y_val)
        )

        params["model_"+str(k)] = "opt: {}, emb_size: {}".format(opt, emb_size)
        k = k + 1

print("Finish Training")

"""# Evaluation"""

print("Modelname -- Parameter")
for k, v in params.items():
  print("{} -- {}".format(k, v))

"""## Plot History Setiap Model"""

list_modelnames = list(history.keys())

dict_acc = {}
dict_val_acc = {}
dict_loss = {}
dict_val_loss = {}

for modelname, modelhistory in history.items():
  dict_acc[modelname] = modelhistory.history['root_mean_squared_error']
  dict_val_acc[modelname] = modelhistory.history['val_root_mean_squared_error']
  dict_loss[modelname] = modelhistory.history['loss']
  dict_val_loss[modelname] = modelhistory.history['val_loss']

label_plot = list_modelnames
marker_plot = ['o','x','v','^']

plt.figure(figsize=(15, 16))
plt.subplot(2, 1, 1)
for i in range(len(list_modelnames)):
  plt.plot(dict_acc[list_modelnames[i]], label=label_plot[i], marker=marker_plot[i], markersize=6, alpha=0.6)

plt.legend(loc='lower right')
plt.ylabel('RMSE')
plt.title('Training RMSE')

plt.subplot(2, 1, 2)
for i in range(len(list_modelnames)):
  plt.plot(dict_val_acc[list_modelnames[i]], label=label_plot[i], marker=marker_plot[i], markersize=6, alpha=0.6)

plt.legend(loc='lower right')
plt.ylabel('RMSE')
plt.title('Validation RMSE')
plt.xlabel('epoch')
plt.savefig('acc_all_model.png')

plt.show()

plt.figure(figsize=(15, 16))
plt.subplot(2, 1, 1)

for i in range(len(list_modelnames)):
  plt.plot(dict_loss[list_modelnames[i]], label=label_plot[i], marker=marker_plot[i], markersize=6, alpha=0.6)

plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.title('Training Loss')

plt.subplot(2, 1, 2)

for i in range(len(list_modelnames)):
  plt.plot(dict_val_loss[list_modelnames[i]], label=label_plot[i], marker=marker_plot[i], markersize=6, alpha=0.6)
  
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.title('Validation Loss')
plt.xlabel('epoch')

plt.savefig('loss_all_model.png')

plt.show()

"""## Membandingkan Metrik Evaluasi Setiap Model"""

# evaluate all model
dict_score_train = {}
dict_score_valid = {}

for modelname, modelresult in model.items():
  dict_score_train[modelname] = modelresult.evaluate(x_train, y_train)
  dict_score_valid[modelname] = modelresult.evaluate(x_val, y_val)

# prepare data before plot
df1 = pd.DataFrame({
    'Model': list_modelnames,
    'Train': [eval[1] for eval in dict_score_train.values()],
    'Valid': [eval[1] for eval in dict_score_valid.values()],
})

df2 = pd.DataFrame({
    'Model': list_modelnames,
    'Train': [eval[0] for eval in dict_score_train.values()],
    'Valid': [eval[0] for eval in dict_score_valid.values()],
})

tidy1 = df1.melt(id_vars='Model').rename(columns=str.title)

tidy2 = df2.melt(id_vars='Model').rename(columns=str.title)

df2

# plot comparison all models evaluate
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))

# first plot
sns.barplot(x='Model', y='Value', hue='Variable', data=tidy1, ax=ax1)
ax1.set_ylabel('RMSE')
ax1.set_ylim([0.125, 0.145])
ax1.set_title('RMSE Performance')

for p, data in zip(ax1.patches, tidy1['Value']):
    ax1.annotate(round(data, 5), xy=(p.get_x()+p.get_width()/2, p.get_height()),
                ha='center', va='bottom')

# second plot
sns.barplot(x='Model', y='Value', hue='Variable', data=tidy2, ax=ax2)
ax2.set_ylabel('Loss')
ax2.set_ylim([0.53, 0.55])
ax2.set_title('Loss Performance')

for p, data in zip(ax2.patches, tidy2['Value']):
    ax2.annotate(round(data, 5), xy=(p.get_x()+p.get_width()/2, p.get_height()),
                ha='center', va='bottom')

plt.savefig('evaluate_all_model.png')
plt.show()

"""model_3 (opt: RMSprop, emb_size: 50) dipilih karena memiliki tingkat RMSE paling rendah dan loss relatif rendah

## Mengambil Rekomendasi Satu User
"""

# Mengambil sample user
user_id = df_rating.user_id.sample(1).iloc[0]
anime_watched_by_user = df_rating[df_rating.user_id == user_id]
 
anime_not_watched = df_anime[~df_anime['anime_id'].isin(anime_watched_by_user.anime_id.values)]['anime_id'] 
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_anime_encoded.keys()))
)
 
anime_not_watched = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

ratings_predict = model["model_3"].predict(user_anime_array).flatten()
 
top_ratings_indices = ratings_predict.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('====' * 10)
print('Top 10 anime with high ratings from user')
print('----' * 10)
 
top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(10)
    .anime_id.values
)
 
df_anime_rows = df_anime[df_anime['anime_id'].isin(top_anime_user)]
for row in df_anime_rows.itertuples():
    print("{} ({}) : {}".format(row.name, row.premiered, row.genres))
 
print('----' * 10)
print('Top 10 anime recommendation')
print('----' * 10)
 
recommended_anime = df_anime[df_anime['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print("{} ({}) : {}".format(row.name, row.premiered, row.genres))

print("anime with high ratings from user")
df_anime_rows

print("Top 10 anime recommendation")
recommended_anime

"""Top genres user likes and recommendation"""

user_genres = []
list_anime_genres = df_anime_rows.genres.unique()

for i, v in enumerate(list_anime_genres):
  user_genres.extend(v.split(', '))

user_genres = sorted(set(user_genres))

total_genre_by_user = {g: 0 for g in user_genres}

for anime_genre in df_anime_rows['genres']:
  list_anime_genre = anime_genre.split(', ')
  for genre in list_anime_genre:
    total_genre_by_user[genre] = total_genre_by_user[genre] + 1

total_genre_by_user = dict(sorted(total_genre_by_user.items(), key=lambda x:x[1], reverse=True))

plt.figure(figsize=(9,5))
sns.barplot(x = list(total_genre_by_user.keys())[:10],
            y = list(total_genre_by_user.values())[:10],
            )
plt.xticks(rotation=90)
plt.title("Diagram Top 10 User Genre Anime")
plt.show()

recommended_genres = []
list_anime_genres = recommended_anime.genres.unique()

for i, v in enumerate(list_anime_genres):
  recommended_genres.extend(v.split(', '))

recommended_genres = sorted(set(recommended_genres))

total_genre_by_recommendation = {g: 0 for g in recommended_genres}

for anime_genre in recommended_anime['genres']:
  list_anime_genre = anime_genre.split(', ')
  for genre in list_anime_genre:
    total_genre_by_recommendation[genre] = total_genre_by_recommendation[genre] + 1

total_genre_by_recommendation = dict(sorted(total_genre_by_recommendation.items(), key=lambda x:x[1], reverse=True))

plt.figure(figsize=(9,5))
sns.barplot(x = list(total_genre_by_recommendation.keys())[:10],
            y = list(total_genre_by_recommendation.values())[:10],
            )
plt.xticks(rotation=90)
plt.title("Diagram Top 10 Recommendation Genre Anime")
plt.show()


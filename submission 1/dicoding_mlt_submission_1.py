# -*- coding: utf-8 -*-
"""Dicoding MLT - Submission 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w_sMYMcdSAEzo26H-Xs_gHSd7FjJ4E8V

# Import Library
"""

! pip install scikeras

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import SVR
import tensorflow as tf
from scikeras.wrappers import KerasRegressor

from sklearn.metrics import mean_squared_error

# %matplotlib inline

"""# Load Dataset"""

# load the dataset from https://www.kaggle.com/datasets/mirichoi0218/insurance
url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'
insurance = pd.read_csv(url)

"""# Data Understanding"""

insurance

"""Deskripsi Variabel

- age: age of primary beneficiary
- sex: insurance contractor gender, female, male
- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
- children: Number of children covered by health insurance / Number of dependents
- smoker: Smoking
- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
- charges: Individual medical costs billed by health insurance
"""

insurance.info()

insurance.describe()

numerical_features = ['age', 'bmi', 'children', 'charges']
categorical_features = ['sex', 'smoker', 'region']

"""## Missing Value"""

insurance.isnull().sum()

age = (insurance.age == 0).sum()
bmi = (insurance.bmi == 0).sum()
 
print("Nilai 0 di kolom age ada: ", age)
print("Nilai 0 di kolom bmi ada: ", bmi)

"""## Data Outlier"""

sns.set()

#define plotting region (2 rows, 2 columns)
fig, axes = plt.subplots(2, 2, figsize=(20,10))

sns.boxplot(x=insurance[numerical_features[0]], ax=axes[0,0])
sns.boxplot(x=insurance[numerical_features[1]], ax=axes[0,1])
sns.boxplot(x=insurance[numerical_features[2]], ax=axes[1,0])
sns.boxplot(x=insurance[numerical_features[3]], ax=axes[1,1])

"""### remove bmi feature"""

Q1 = insurance['bmi'].quantile(0.25)
Q3 = insurance['bmi'].quantile(0.75)
IQR=Q3-Q1

print("bmi Data Outlier:")
insurance[(insurance['bmi'] < (Q1-1.5*IQR)) | (insurance['bmi'] > (Q3+1.5*IQR))]

index = insurance[(insurance['bmi'] < (Q1-1.5*IQR)) | (insurance['bmi'] > (Q3+1.5*IQR))].index
insurance = insurance.drop(index=index)

"""### remove some charges data"""

insurance.sort_values(by=['charges'], ascending=False).head(10)

# remove charges data above 50000
index = insurance[insurance['charges'] > 50000].index
insurance = insurance.drop(index=index)

insurance.shape

"""## Univariate Analysis"""

# numerical feature
insurance.hist(bins=50, figsize=(15,7))
plt.show()

# children feature
feature = numerical_features[2]
count = insurance[feature].value_counts()
percent = 100*insurance[feature].value_counts(normalize=True)
df = pd.DataFrame({'total':count, 'percentage':percent.round(1)})
print(df)

sns.set(style="darkgrid")
sns.barplot(x=insurance[feature].value_counts().keys(), y=count)
plt.show()

# sex feature
feature = categorical_features[0]
count = insurance[feature].value_counts()
percent = 100*insurance[feature].value_counts(normalize=True)
df = pd.DataFrame({'total':count, 'percentage':percent.round(1)})
print(df)

sns.set(style="darkgrid")
sns.barplot(x=insurance[feature].value_counts().keys(), y=count)
plt.show()

# smoker feature
feature = categorical_features[1]
count = insurance[feature].value_counts()
percent = 100*insurance[feature].value_counts(normalize=True)
df = pd.DataFrame({'total':count, 'percentage':percent.round(1)})
print(df)

sns.set(style="darkgrid")
sns.barplot(x=insurance[feature].value_counts().keys(), y=count)
plt.show()

# region feature
feature = categorical_features[2]
count = insurance[feature].value_counts()
percent = 100*insurance[feature].value_counts(normalize=True)
df = pd.DataFrame({'total':count, 'percentage':percent.round(1)})
print(df)

sns.set(style="darkgrid")
sns.barplot(x=insurance[feature].value_counts().keys(), y=count)
plt.show()

"""## Multivariate Analysis"""

for col in categorical_features:
  sns.catplot(x=col, y="charges", kind="bar", dodge=False, height = 4, aspect = 3, data=insurance)
  plt.title("Rata-rata 'charges' Relatif terhadap - {}".format(col))

sns.pairplot(insurance, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = insurance.corr().round(2)
 
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

# drop children feature because it has low corelation (0.07)
insurance.drop(['children'], inplace=True, axis=1)
insurance.head()

"""# Data Preparation

## Encoding
"""

# encode sex feature
enc = OrdinalEncoder()
enc_sex = enc.fit_transform(insurance['sex'].values.reshape(-1,1))
insurance['sex'] = enc_sex.reshape(1, -1)[0].astype(int)

# encode smoker feature
enc = OrdinalEncoder()
enc_smoker = enc.fit_transform(insurance['smoker'].values.reshape(-1,1))
insurance['smoker'] = enc_sex.reshape(1, -1)[0].astype(int)

# encode region feature
insurance = pd.concat([insurance, pd.get_dummies(insurance['region'], prefix='region')],axis=1)
insurance.drop(['region'], axis=1, inplace=True)

insurance.head()

"""## Splitting Dataset"""

X = insurance.drop(["charges"],axis=1)
y = insurance["charges"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""## Standardization"""

numerical_features = ['age', 'bmi']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])

X_train.head()

"""# Model Development

## K Nearest Neighbors
"""

knn_params = {'n_neighbors': [1, 3, 6, 10, 15, 21]}
knn_estimator = KNeighborsRegressor()
knn = GridSearchCV(estimator=knn_estimator, param_grid=knn_params, scoring='neg_mean_squared_error')
knn.fit(X_train, y_train)

print("Best Estimator:", knn.best_estimator_)
print("Best Score:", knn.best_score_)

means = knn.cv_results_['mean_test_score']
stds = knn.cv_results_['std_test_score']
params = knn.cv_results_['params']
print("mean_test_score | std_test_score with: params")
for mean, stdev, param in zip(means, stds, params):
    print("%f | (%f) with: %r" % (mean, stdev, param))

"""## Random Forest"""

RF_params = {
    'n_estimators': [30, 50, 100, 150, 200],
    'max_depth': [None, 16, 32, 64]
    }
RF_estimator = RandomForestRegressor(random_state=42, n_jobs=-1)
RF = GridSearchCV(estimator=RF_estimator, param_grid=RF_params, scoring='neg_mean_squared_error')
RF.fit(X_train, y_train)

print("Best Estimator:", RF.best_estimator_)
print("Best Score:", RF.best_score_)

means = RF.cv_results_['mean_test_score']
stds = RF.cv_results_['std_test_score']
params = RF.cv_results_['params']
print("mean_test_score | std_test_score with: params")
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

"""## AdaBoost"""

boosting_params = {
    'n_estimators': [15, 30, 50, 100],
    'learning_rate': [0.005, 0.05, 0.5, 1]
    }
boosting_estimator = AdaBoostRegressor(random_state=42)
boosting = GridSearchCV(estimator=boosting_estimator, param_grid=boosting_params, scoring='neg_mean_squared_error')
boosting.fit(X_train, y_train)

print("Best Estimator:", boosting.best_estimator_)
print("Best Score:", boosting.best_score_)

means = boosting.cv_results_['mean_test_score']
stds = boosting.cv_results_['std_test_score']
params = boosting.cv_results_['params']
print("mean_test_score | std_test_score with: params")
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

"""## Support Vector Machine"""

svm_params = {
    'kernel': ['rbf', 'sigmoid'],
    'C': [1, 3, 6, 10, 15, 20]
    }
svm_estimator = SVR()
svm = GridSearchCV(estimator=svm_estimator, param_grid=svm_params, scoring='neg_mean_squared_error')
svm.fit(X_train, y_train)

print("Best Estimator:", svm.best_estimator_)
print("Best Score:", svm.best_score_)

means = svm.cv_results_['mean_test_score']
stds = svm.cv_results_['std_test_score']
params = svm.cv_results_['params']
print("mean_test_score | std_test_score with: params")
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

"""## Neural Network Tensorflow"""

tf.random.set_seed(42)

def create_model():
	tensor = tf.keras.Sequential([
      tf.keras.layers.Dense(16, activation='relu', input_shape=[8,]),
      tf.keras.layers.Dense(16, activation='relu'),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(1),
  ])
	return tensor

tensor_estimator = KerasRegressor(model=create_model, verbose=0)

tensor_params = {
    'optimizer': ['RMSprop', 'Adam'],
    'loss': ['mse', 'huber'],
    'epochs': [50, 100, 150]
}

tensor = GridSearchCV(estimator=tensor_estimator, param_grid=tensor_params, scoring='neg_mean_squared_error', n_jobs=-1)
tensor.fit(X_train, y_train)

print("Best Estimator:", tensor.best_estimator_)
print("Best Score:", tensor.best_score_)

means = tensor.cv_results_['mean_test_score']
stds = tensor.cv_results_['std_test_score']
params = tensor.cv_results_['params']
print("mean_test_score | std_test_score with: params")
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

"""# Evaluation"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting', 'SVM', 'Tensorflow'])
 
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting, 'SVM': svm, 'Tensorflow': tensor}
 
for name, model in model_dict.items():
  mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
  mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

np.random.seed(13)

int_rand = np.random.randint(0, 100)

prediksi = X_test.iloc[int_rand:int_rand+2].copy()
pred_dict = {'y_true':y_test[int_rand:int_rand+2]}

for name, model in model_dict.items():
  pred_dict['prediksi_'+name] = model.predict(prediksi).round(1).reshape(1, -1)[0]
 
pd.DataFrame(pred_dict)

